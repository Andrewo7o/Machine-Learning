# Mashine-Learning
In this repository,  I will sort out some knowledge points about machine learning.
极大似然估计：模型已定，参数未知
极大似然估计其实是理想地认为，对于极少的样本观测，观测到的样本很可能就是概率最大的
           样本较少时，极大似然估计偏差较大，但随着样本的增多，偏差慢慢趋于0
           用局部估计整体时，可以说运用了极大似然估计法，也可以说根据大数定律     
矩估计的优点是简单空，只需知道总体的矩，总体分布形式不必知道，而极大似然估计必须知道总体分布形式
贝叶斯估计：从参数的先验知识和样本出发，将待估计的参数看做符合某种先验概率分布的随机变量
大数定律不会对已经发生的情况进行平衡，而是利用新的数据来削弱它的影响

最小二乘法主要思想是求解未知参数，使得理论值与观测值之差的平方和达到最小
正则化就是给需要训练的目标函数加上一些规则（限制），让他们不要自我膨胀（过拟合）
正则化方法会自动削弱不重要的特征变量，自动从许多的特征变量中“提取”重要的特征变量，减少特征变量的数据级

L0范数是指向量中非零元素的个数
L1范数是指向量中每个元素绝对值的和
L2范数是指向量元素绝对值的平方和再开平方
L2正则化使用一个乘性因子(< 1)去调整权重,因此权重会不断衰减，并且在权重较大时衰减地快，在权重较小时衰减地慢
正则化让模型根据训练数据中常见的模式来学习相对简单的模型，无正则化的模型用大参数学习到了较大的噪声
L2正则化通过权重衰减，保证了模型的简单，提高了泛化能力
L1正则化通过加上或减去一个常量值，让权重值w向0靠近
L1正则化对小权重减小得很快，对大权重减小缓慢，因此最终模型的权重主要集中在高重要度的特征上，对于相对不重要的特征，权重会很快趋于0
