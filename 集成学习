# Mashine-Learning
In this repository,  I will sort out some knowledge points about machine learning.

集成学习（Ensemble Learning）

同质集成中的个体学习器称作“基学习器”（base learner），相应的学习算法称为基学习算法（base learning algorithm）
异质集成中的个体学习器称作“组件学习器”（component learner），或直接称为个体学习器

弱学习器：指泛化性能略优于随机猜测的学习器
随着集成中个体分类器数目T的增大，集成的错误率将指数级下降，最终趋向于零
集成学习研究的核心：如何产生并结合“好而不同”的个体学习器

A.个体学习器间存在强依赖关系，必须串行生成的序列化方法：Boosting
B.个体学习器不存在强依赖关系，可同时生成的并行化方法：Bagging和“随机森林”

Boosting --> AdaBoost "残差逼近"思想，主要关注的是降低偏差，Boosting能基于泛化性能相当弱的学习器构建出很强的集成
Bagging --> 对分类任务使用简单投票法，对回归任务使用简单平均法，主要关注的是降低方差    “自助采样法”使得“包外样本”辅助剪枝
（RF）随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择（样本扰动+属性扰动）
随机森林的训练效率优于Bagging

结合策略：
A.平均法--加权平均法 个体学习器性能差别较大是宜使用加权平均，个体学习器性能相近时，宜使用简单平均法
B.投票法--a.绝对多数投票法 b.相对多数投票法 c.加权投票法
C.学习法--典型代表：Stacking 将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归（Multi-response Linear Regression，MLR）
  作为次级学习算法的效果最好
  MLR是基于线性回归的分类器，对每个类分别进行线性回归，属于该类的训练样例所对应的输出值为1，其他类为0，测试示例将被分给输出值最大的类
  
多样性--个体学习器准确性越高，多样性越大，则集成效果越好（好而不同） 理论支持来自“误差-分歧分解”
多样性度量--估算个体学习器的多样化程度，典型做法是考虑个体分类器的两两相似/不相似性
           a.不合度量 b.相关系数 c.Q-统计量 d.k-统计量
多样性增强：
A.数据样本扰动--Bagging的自助采样，AdaBoost的序列采样
B.输入属性扰动--随机子空间算法（random subspace） 若数据只包含少量属性，或者冗余属性很少，则不宜用输入属性扰动法
C.输出表示扰动--翻转法（随即改变训练样本标记）、输出调制法（分类-->回归）、ECOC法（多分类-->多个二分类）
D.算法参数扰动--通过随即设置不同的参数，往往可产生差别较大的个体学习器


