# Mashine-Learning
In this repository,  I will sort out some knowledge points about machine learning.
Word2Vec：通过学习文本来用词向量的方式表征词的语义信息
          即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近
Skip-Gram: 给定input word来预测上下文
           训练过程 --> 输出层以ants的词向量为特征，以ants的上下文作为类别训练softmax分类器
CBOW(连续词袋模型)：给定上下文预测input word
Embedding是一个映射，将单词从原先所属空间映射到新的多维空间，将原先词所在空间嵌入到一个新的空间中
"Fake Task", 意味着建模并不是最终的目的，最终目标是学习隐层的权重矩阵
             隐层权重矩阵 --> "查找表" 隐层的输出就是每个输入单词的嵌入词向量
A.skip_windows: 代表从当前input word的一侧选取词的数量
B.num_skips:代表从整个窗口中选取多少不同的词作为output word

自动编码器：一种数据的压缩算法
           a.数据相关 意味着自动编码器只能压缩那些与训练数据类似的数据
           b.有损压缩 解压缩的输出与原来的输入相比是退化的
           c.从样本中自动学习 对指定类的输入训练出一种特定编码器
自编码器的应用： a.数据去噪  b.进行可视化的降维
学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征
正则自编码器：
a.稀疏自编码器 --> 稀疏惩罚 + 重构误差   目的是得到更紧缩的表达
b.去噪自编码器 --> (denoising auto encoder, DAE) 最小化 L(x,g(f(˜x))), 其中 ˜x 是被某种噪声损坏的 x 的副本
c.收缩自编码器 --> (contractive auto encoder, CAE) 
                  迫使模型在学习一个 x 变化小时目标也没有太大变化的函数，将输入的邻域缩小到更小的输出邻域

反向传播算法(BP算法)： 本质是随机梯度下降，链式求导法则而来的
层级softmax: 在模型训练时，首先统计语料中词语的词频，然后根据词频构建赫夫曼树
             使用二元赫夫曼树，因为它将短代码分配给频繁词汇可使得训练速度加快
为什么赫夫曼树可以加快训练速度？
    输出层不使用one-hot来表示，softmax回归就不需要对那么多0(也即负样本)进行拟合，仅仅只需要拟合输出值在Huffman树中的一条路径
              
词干化(stemming)：去除词缀得到词根的过程
batch：批处理，也称批处理脚本  将分析过程转换为代码 --> 找到每个input word的上下文，基于上下文构建batch
在隐藏层中，每次只有输入词的权重更新 
要求模型在像素级上精确重构输入不是机器学习的兴趣所在，学习到高级抽象才是

一般讲来，每个训练样本将调整神经网络的所有权重，所以引进负采样(nagative sampling): 1个positive + n个随机负样本，使计算效率大幅度提高
                                                                              词语出现的概率越高，其被采样到的概率越低

