# Mashine-Learning
In this repository,  I will sort out some knowledge points about machine learning.
对偶问题：实质相同但从不同角度提出不同提法的一对问题
使用对偶问题的优点： a.对偶问题往往更容易求解
                    b.可以引进核函数，进而推广到非线性分类问题

那些距离分割平面最近的点就是支持向量(Support Vectors)
支持向量机训练完成后，大部分训练样本都不需保留，最终模型仅与支持向量有关
训练过程：在数据点中找到距离分割平面最近的点(支持向量)，然后优化w和b来最大化支持向量到分割超平面的距离
由于可以等比例的改变参数w和b，函数间隔相当于几何间隔上的参数w和b都乘上了||w||,几何间隔就是分类样本点到超平面的距离
KTT条件是一个非线性规划问题能有最优化解法的一个充分必要条件

如果原始空间是有限维，那么一定存在一个高维特征空间使样本可分
对于非线性可分的数据，通常需要将数据映射到高维空间中使得原本在低维空间线性不可分的数据在高维空间中线性可分
当维度太高时，计算内积的复杂度会很高，此时需要核函数来拯救计算复杂度
"核化"(即引入核函数)来将线性学习器拓展为非线性学习器
核函数来代替向量内积,核函数可以表示成向量内积的形式，但在计算结果时直接求函数值就好，不需要做内积运算
核函数类似于一个黑匣子，在低维计算，将分类结果表现在高维
      不需要显式地将输入空间中样本映射到新空间，而能够在输入空间中直接进行内积运算
高斯核函数(Gaussian kernel)能够将数据映射到无限维空间，无限维空间中数据都是线性可分的

所有样本都必须划分正确，称为"硬间隔"，"软间隔"允许某些样本不满足约束
L1软间隔SVM(L1 soft margin SVM): 松弛变量 + 惩罚函数

坐标上升算法(Coordinate Ascent): 每次通过更新多元函数中的一维，经过多次迭代直到收敛来达到优化函数的目的
SMO算法(Sequential Minimal Optimization)，一种启发式算法
                                       基本思路是一次迭代只优化两个变量而固定剩余的变量,就是将一个大的优化问题分解为若干个小的易于求解的优化问题
                                       
支持向量回归(SVR, Support Vector Regression)假设能容忍f(x)与y之间最多ε的偏差，当两个量差的绝对值大于ε时才会计算损失
                                       
