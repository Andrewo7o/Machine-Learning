# Mashine-Learning
In this repository,  I will sort out some knowledge points about machine learning.
非线性模型（nonlinear model）可在线性模型的基础上通过引入层级结构或高维映射而得。
"线性回归"试图学得一个线性模型以尽可能准确地预测实值输出标记
"均方误差"是回归任务中最常用的性能度量，试图让均方误差最小化
基于均方误差最小化来进行模型求解的方法称"最小二乘法"
在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离最小
求解w和b的过程，称为线性回归模型的最小二乘"参数估计"

正则化可以理解为一种"罚函数法"，即对不希望得到的结果施以惩罚，从而使优化过程趋向于希望目标，从贝叶斯估计的角度来看，正则化项可认为是提供了模型的先验概率
对数线性回归 <-- 非线性函数映射    更一般地，考虑单调可微函数 <-- 广义线性模型
对数几率函数（logistic function），一种 "Sigmoid" 函数
线性判别分析（Linear Discriminant Analysis，LDA） <--> Fisher判别分析
思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离
欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小
于是异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大
LDA也常被视为一种经典的监督降维技术

多分类学习的基本思路是"拆解法"，即将多分类任务拆为若干个二分类任务求解
拆分策略：
A.一对一 OvO   OvR只需训练N个分类器，而OvO需要训练 N(N-1)/2 个训练器
B.一对其余 OvR  OvO的存储开销和测试时间开销通常比OvR大，但OvO的训练时间开销通常比OvR小
C.多对多 MvM  --> 纠错输出码（Error Correcting Output Codes，ECOC）
                  ECOC编码对分类器的错误有一点容忍和修正能力，并不需要获得理论最优编码，因为非最优编码在实践中往往已经能够产生足够好的分类器
                  
类别不平衡：分类任务中不同类别的训练样例数目差别很大
基本策略是 -- "再缩放"：
A."欠采样"：删反例，使正例反例数目相近
B."过采样"：添正例，使正例反例数目相近
C.阈值移动
"再缩放"策略是"代价敏感学习"的基础

如果希望为一个样本同时预测多个标记，就不再是多分类学习，而是"多标记学习"
